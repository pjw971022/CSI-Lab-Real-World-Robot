{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bef37d",
   "metadata": {},
   "source": [
    "# Affordance Heatmaps\n",
    "\n",
    "This notebook visualizes the pick and place affordance predictions of a pre-trained `multi-language-conditioned` agent from the quickstart guide.\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Set the root folder environment variable with `export CLIPORT_ROOT=<cliport_root>`\n",
    "- Complete the [quickstart guide](https://github.com/cliport/cliport#quickstart) in README.md\n",
    "- Generate `val` and `test` splits for the task you want to evaluate on by running `python cliport/demos.py n=10 mode=test task=stack-block-pyramid-seq-seen-colors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca5b47c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: RAVENS_ROOT=/home/pjw971022/RealWorldLLM/ravens/\n"
     ]
    }
   ],
   "source": [
    "# set GPU\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env RAVENS_ROOT=/home/pjw971022/RealWorldLLM/ravens/\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('/home/pjw971022/RealWorldLLM/cliport/')\n",
    "import numpy as np\n",
    "from cliport import tasks\n",
    "from cliport import agents\n",
    "from cliport.utils import utils\n",
    "from cliport.environments.environment_real import RealEnvironment\n",
    "import torch\n",
    "# sys.path.append(os.environ['RAVENS_ROOT'])\n",
    "# from ravens import tasks\n",
    "# from ravens.environments.environment import Environment\n",
    "# from ravens.dataset import RavensDataset\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6b5a6",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d00e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_demos = 1000 # number training demonstrations used to train agent\n",
    "n_eval = 1 # number of evaluation instances\n",
    "mode = 'test' # val or test\n",
    "\n",
    "agent_name = 'cliport'\n",
    "model_task = 'multi-language-conditioned' # multi-task agent conditioned with language goals\n",
    "\n",
    "model_folder = 'cliport_quickstart' # path to pre-trained checkpoint\n",
    "ckpt_name = 'last.ckpt' # name of checkpoint to load\n",
    "\n",
    "draw_grasp_lines = True\n",
    "affordance_heatmap_scale = 30\n",
    "\n",
    "### Uncomment the task you want to evaluate on ###\n",
    "# eval_task = 'align-rope'\n",
    "# eval_task = 'assembling-kits-seq-seen-colors'\n",
    "# eval_task = 'assembling-kits-seq-unseen-colors'\n",
    "# eval_task = 'packing-shapes'\n",
    "# eval_task = 'packing-boxes-pairs-seen-colors'\n",
    "# eval_task = 'packing-boxes-pairs-unseen-colors'\n",
    "# eval_task = 'packing-seen-google-objects-seq'\n",
    "# eval_task = 'packing-unseen-google-objects-seq'\n",
    "# eval_task = 'packing-seen-google-objects-group'\n",
    "# eval_task = 'packing-unseen-google-objects-group'\n",
    "# eval_task = 'put-block-in-bowl-seen-colors'\n",
    "# eval_task = 'put-block-in-bowl-unseen-colors'\n",
    "# eval_task = 'stack-block-pyramid-seq-seen-colors'\n",
    "# eval_task = 'stack-block-pyramid-seq-unseen-colors'\n",
    "# eval_task = 'separating-piles-seen-colors'\n",
    "# eval_task = 'separating-piles-unseen-colors'\n",
    "# eval_task = 'towers-of-hanoi-seq-seen-colors'\n",
    "# eval_task = 'towers-of-hanoi-seq-unseen-colors'\n",
    "eval_task = 'real-world-cleanup'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812da35",
   "metadata": {},
   "source": [
    "### Load Configs and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0042f541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval ID: real-world-cleanup-cliport-1-0\n",
      "\n",
      "Attn FCN - Stream One: plain_resnet_lat, Stream Two: clip_lingunet_lat, Stream Fusion: add\n",
      "Transport FCN - Stream One: plain_resnet_lat, Stream Two: clip_lingunet_lat, Stream Fusion: conv\n",
      "Agent: real-world-cleanup-cliport-1-0, Logging: False\n",
      "\n",
      "Loading checkpoint: /home/pjw971022/RealWorldLLM/cliport/cliport_quickstart/multi-language-conditioned-cliport-n1000-train/checkpoints/last.ckpt\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/pjw971022/RealWorldLLM/cliport/'\n",
    "assets_root = os.path.join(root_dir, 'cliport/environments/assets/')\n",
    "config_file = 'eval.yaml' \n",
    "\n",
    "vcfg = utils.load_hydra_config(os.path.join(root_dir, f'cliport/cfg/{config_file}'))\n",
    "vcfg['data_dir'] = '/home/mnt/data/ravens/'\n",
    "vcfg['mode'] = mode\n",
    "\n",
    "vcfg['model_task'] = model_task\n",
    "vcfg['eval_task'] = eval_task\n",
    "vcfg['agent'] = agent_name\n",
    "\n",
    "# Model and training config paths\n",
    "model_path = os.path.join(root_dir, model_folder)\n",
    "vcfg['train_config'] = f\"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/.hydra/config.yaml\"\n",
    "vcfg['model_path'] = f\"{model_path}/{vcfg['model_task']}-{vcfg['agent']}-n{train_demos}-train/checkpoints/\"\n",
    "\n",
    "tcfg = utils.load_hydra_config(vcfg['train_config'])\n",
    "\n",
    "# Load dataset\n",
    "# ds = RavensDataset(os.path.join(vcfg['data_dir'], f'{vcfg[\"eval_task\"]}-{vcfg[\"mode\"]}'), \n",
    "#                    tcfg, \n",
    "#                    n_demos=n_eval,\n",
    "#                    augment=False)\n",
    "\n",
    "eval_run = 0\n",
    "name = '{}-{}-{}-{}'.format(vcfg['eval_task'], vcfg['agent'], n_eval, eval_run)\n",
    "print(f'\\nEval ID: {name}\\n')\n",
    "\n",
    "# Initialize agent\n",
    "utils.set_seed(eval_run, torch=True)\n",
    "agent = agents.names[vcfg['agent']](name, tcfg, None, None)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = os.path.join(vcfg['model_path'], ckpt_name)\n",
    "print(f'\\nLoading checkpoint: {ckpt_path}')\n",
    "agent.load(ckpt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6832c0",
   "metadata": {},
   "source": [
    "### Spawn Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a0da7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROS Client Start\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cliport.tasks.cameras' has no attribute 'RealSenseD435'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcliport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cameras\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m RealEnvironment(\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m CAMERA_CONFIG \u001b[38;5;241m=\u001b[39m \u001b[43mcameras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRealSenseD435\u001b[49m\u001b[38;5;241m.\u001b[39mCONFIG\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_image\u001b[39m(obs, cam_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stack color and height images image.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cliport.tasks.cameras' has no attribute 'RealSenseD435'"
     ]
    }
   ],
   "source": [
    "# Initialize environment and task.\n",
    "from cliport.tasks import cameras\n",
    "\n",
    "env = RealEnvironment(\n",
    ")\n",
    "CAMERA_CONFIG = cameras.RealSenseD435.CONFIG\n",
    "def get_image(obs, cam_config=None):\n",
    "    \"\"\"Stack color and height images image.\"\"\"\n",
    "    bounds = np.array([[0.25, 0.75], [-0.5, 0.5], [-1, 1]])\n",
    "    #bounds = np.array([[0.25, 0.75], [-0.5, 0.5], [0, 0.28]])\n",
    "    pix_size = 0.003125 #* 2\n",
    "    in_shape = (320, 160, 6)\n",
    "    # if self.use_goal_image:\n",
    "    #   colormap_g, heightmap_g = utils.get_fused_heightmap(goal, configs)\n",
    "    #   goal_image = self.concatenate_c_h(colormap_g, heightmap_g)\n",
    "    #   input_image = np.concatenate((input_image, goal_image), axis=2)\n",
    "    #   assert input_image.shape[2] == 12, input_image.shape\n",
    "\n",
    "    if cam_config is None:\n",
    "        cam_config = CAMERA_CONFIG\n",
    "\n",
    "    # Get color and height maps from RGB-D images.\n",
    "    cmap, hmap = utils.get_fused_heightmap_real(\n",
    "        obs, cam_config, bounds, pix_size)\n",
    "    img = np.concatenate((cmap,\n",
    "                            hmap[Ellipsis, None],\n",
    "                            hmap[Ellipsis, None],\n",
    "                            hmap[Ellipsis, None]), axis=2)\n",
    "    assert img.shape == in_shape, img.shape\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c138c4a",
   "metadata": {},
   "source": [
    "### Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b86bb611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Instance: 1/1\n",
      "Task name:  real-world-cleanup\n",
      "<module 'cliport.tasks' from '/home/pjw971022/RealWorldLLM/cliport/cliport/tasks/__init__.py'>\n"
     ]
    },
    {
     "ename": "ZMQError",
     "evalue": "Operation cannot be accomplished in current state",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZMQError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m env\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     24\u001b[0m env\u001b[38;5;241m.\u001b[39mset_task(task)\n\u001b[0;32m---> 25\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m     27\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/RealWorldLLM/cliport/cliport/environments/environment_real.py:88\u001b[0m, in \u001b[0;36mRealEnvironment.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 88\u001b[0m     obs, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n",
      "File \u001b[0;32m~/RealWorldLLM/cliport/cliport/environments/environment_real.py:121\u001b[0m, in \u001b[0;36mRealEnvironment.step\u001b[0;34m(self, raw_action)\u001b[0m\n\u001b[1;32m    119\u001b[0m obs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m: (), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m: ()}\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_cams:\n\u001b[0;32m--> 121\u001b[0m     color, depth, pointcloud \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_camera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (color,)\n\u001b[1;32m    123\u001b[0m     obs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (depth,)\n",
      "File \u001b[0;32m~/RealWorldLLM/cliport/cliport/environments/environment_real.py:150\u001b[0m, in \u001b[0;36mRealEnvironment.render_camera\u001b[0;34m(self, config, image_size)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender_camera\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# render_camera\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(data)\n\u001b[1;32m    153\u001b[0m     color \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/omni/lib/python3.9/site-packages/zmq/sugar/socket.py:934\u001b[0m, in \u001b[0;36mSocket.recv_string\u001b[0;34m(self, flags, encoding)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecv_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, flags: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, encoding: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    915\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Receive a unicode string, as sent by send_string.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m        for any of the reasons :func:`~Socket.recv` might fail\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 934\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(msg, \u001b[38;5;28;01mlambda\u001b[39;00m buf: buf\u001b[38;5;241m.\u001b[39mdecode(encoding))\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:805\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:841\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:199\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mzmq/backend/cython/socket.pyx:194\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/omni/lib/python3.9/site-packages/zmq/backend/cython/checkrc.pxd:28\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mZMQError\u001b[0m: Operation cannot be accomplished in current state"
     ]
    }
   ],
   "source": [
    "episode = 0\n",
    "num_eval_instances = 1 # min(n_eval, ds.n_episodes)\n",
    "\n",
    "for i in range(num_eval_instances):\n",
    "    print(f'\\nEvaluation Instance: {i + 1}/{num_eval_instances}')\n",
    "    \n",
    "    # Load episode\n",
    "    # episode, seed = ds.load(i)\n",
    "    seed = 0\n",
    "    goal = None #episode[-1]\n",
    "    total_reward = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set task\n",
    "    task_name = vcfg['eval_task']\n",
    "    print(\"Task name: \", task_name)\n",
    "    # print(f\"State:{env.task.lang_initial_state}\")\n",
    "    task = tasks.names[task_name]()\n",
    "    print(tasks)\n",
    "    task.mode = mode\n",
    "    \n",
    "    # Set environment\n",
    "    env.seed(seed)\n",
    "    env.set_task(task)\n",
    "    obs = env.reset()\n",
    "    info = env.info\n",
    "    reward = 0\n",
    "    \n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    # Rollout\n",
    "    while (step <= task.max_steps) and not done:\n",
    "        print(f\"Step: {step} ({task.max_steps} max)\")\n",
    "        \n",
    "        # Get batch\n",
    "        # if step == task.max_steps-1:\n",
    "        #     batch = ds.process_goal((obs, None, reward, info), perturb_params=None)\n",
    "        # else:\n",
    "        #     batch = ds.process_sample((obs, None, reward, info), augment=False)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(13, 7))\n",
    "        \n",
    "        # Get color and depth inputs\n",
    "        img = get_image(obs) # batch['img']\n",
    "        img = torch.from_numpy(img)\n",
    "        color = np.uint8(img.detach().cpu().numpy())[:,:,:3]\n",
    "        color = color.transpose(1,0,2)\n",
    "        depth = np.array(img.detach().cpu().numpy())[:,:,3]\n",
    "        depth = depth.transpose(1,0)\n",
    "        \n",
    "        # Display input color\n",
    "        axs[0,0].imshow(color)\n",
    "        axs[0,0].axes.xaxis.set_visible(False)\n",
    "        axs[0,0].axes.yaxis.set_visible(False)\n",
    "        axs[0,0].set_title('Input RGB')\n",
    "        \n",
    "        # Display input depth\n",
    "        axs[0,1].imshow(depth)\n",
    "        axs[0,1].axes.xaxis.set_visible(False)\n",
    "        axs[0,1].axes.yaxis.set_visible(False)        \n",
    "        axs[0,1].set_title('Input Depth')\n",
    "        \n",
    "        # Display predicted pick affordance\n",
    "        axs[1,0].imshow(color)\n",
    "        axs[1,0].axes.xaxis.set_visible(False)\n",
    "        axs[1,0].axes.yaxis.set_visible(False)\n",
    "        axs[1,0].set_title('Pick Affordance')\n",
    "        \n",
    "        # Display predicted place affordance\n",
    "        axs[1,1].imshow(color)\n",
    "        axs[1,1].axes.xaxis.set_visible(False)\n",
    "        axs[1,1].axes.yaxis.set_visible(False)\n",
    "        axs[1,1].set_title('Place Affordance')\n",
    "        \n",
    "        # Get action predictions\n",
    "        l = str(info['lang_goal'])\n",
    "        act = agent.act(img, info, goal=None)\n",
    "        pick, place = act['pick'], act['place']\n",
    "        \n",
    "        # Visualize pick affordance\n",
    "        pick_inp = {'inp_img': img, 'lang_goal': l}\n",
    "        pick_conf = agent.attn_forward(pick_inp)\n",
    "        logits = pick_conf.detach().cpu().numpy()\n",
    "\n",
    "        pick_conf = pick_conf.detach().cpu().numpy()\n",
    "        argmax = np.argmax(pick_conf)\n",
    "        argmax = np.unravel_index(argmax, shape=pick_conf.shape)\n",
    "        p0 = argmax[:2]\n",
    "        p0_theta = (argmax[2] * (2 * np.pi / pick_conf.shape[2])) * -1.0\n",
    "    \n",
    "        line_len = 30\n",
    "        pick0 = (pick[0] + line_len/2.0 * np.sin(p0_theta), pick[1] + line_len/2.0 * np.cos(p0_theta))\n",
    "        pick1 = (pick[0] - line_len/2.0 * np.sin(p0_theta), pick[1] - line_len/2.0 * np.cos(p0_theta))\n",
    "\n",
    "        if draw_grasp_lines:\n",
    "            axs[1,0].plot((pick1[0], pick0[0]), (pick1[1], pick0[1]), color='r', linewidth=1)\n",
    "        \n",
    "        # Visualize place affordance\n",
    "        # l = ['move the cyan ring to the middle of the stand' , 'move the yellow ring to the middle of the stand', 'move the gray ring to the middle of the stand']\n",
    "        place_inp = {'inp_img': img, 'p0': pick, 'lang_goal': l}\n",
    "        place_conf = agent.trans_forward(place_inp)\n",
    "\n",
    "        place_conf = place_conf.permute(1, 2, 0)\n",
    "        place_conf = place_conf.detach().cpu().numpy()\n",
    "        argmax = np.argmax(place_conf)\n",
    "        argmax = np.unravel_index(argmax, shape=place_conf.shape)\n",
    "        p1_pix = argmax[:2]\n",
    "        p1_theta = (argmax[2] * (2 * np.pi / place_conf.shape[2]) + p0_theta) * -1.0\n",
    "        \n",
    "        line_len = 30\n",
    "        place0 = (place[0] + line_len/2.0 * np.sin(p1_theta), place[1] + line_len/2.0 * np.cos(p1_theta))\n",
    "        place1 = (place[0] - line_len/2.0 * np.sin(p1_theta), place[1] - line_len/2.0 * np.cos(p1_theta))\n",
    "\n",
    "        if draw_grasp_lines:\n",
    "            axs[1,1].plot((place1[0], place0[0]), (place1[1], place0[1]), color='g', linewidth=1)\n",
    "        \n",
    "        # Overlay affordances on RGB input\n",
    "        pick_logits_disp = np.uint8(logits * 255 * affordance_heatmap_scale).transpose(1,0,2)\n",
    "        place_logits_disp = np.uint8(np.sum(place_conf, axis=2)[:,:,None] * 255 * affordance_heatmap_scale).transpose(1,0,2)    \n",
    "\n",
    "        pick_logits_disp_masked = np.ma.masked_where(pick_logits_disp < 0, pick_logits_disp)\n",
    "        place_logits_disp_masked = np.ma.masked_where(place_logits_disp < 0, place_logits_disp)\n",
    "\n",
    "        axs[1][0].imshow(pick_logits_disp_masked, alpha=0.75)\n",
    "        axs[1][1].imshow(place_logits_disp_masked, cmap='viridis', alpha=0.75)\n",
    "        \n",
    "        print(f\"Lang Goal: {str(info['lang_goal'])}\")\n",
    "        plt.savefig(f'/home/pjw970122/RealWorldLLM/real_bot/save_viz/cliport_only/heatmap/{eval_task}_{step}_init.png')\n",
    "        \n",
    "        # Act with the predicted actions\n",
    "        import ipdb;ipdb.set_trace()\n",
    "        # obs, reward, done, info = env.step(act)\n",
    "        step += 1\n",
    "        \n",
    "    if done:\n",
    "        print(\"Done. Success.\")\n",
    "    else:\n",
    "        print(\"Max steps reached. Task failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f76a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue ring on top of green ring. green ring on top of brown ring. brown ring in lighter brown side. The rings can be moved in lighter brown side, middle of the stand, darker brown side. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.task.lang_initial_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
