{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPrCVnimE0qR"
      },
      "source": [
        "## Load pre-trained model and processor\n",
        "\n",
        "Let's first apply the image preprocessing and tokenize the text queries using `OwlViTProcessor`. The processor will resize the image(s), scale it between [0-1] range and normalize it across the channels using the mean and standard deviation specified in the original codebase.\n",
        "\n",
        "\n",
        "Text queries are tokenized using a CLIP tokenizer and stacked to output tensors of shape [batch_size * num_max_text_queries, sequence_length]. If you are inputting more than one set of (image, text prompt/s), num_max_text_queries is the maximum number of text queries per image across the batch. Input samples with fewer text queries are padded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD8DXCnJ7faH"
      },
      "outputs": [],
      "source": [
        "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "\n",
        "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\", do_pad = False)\n",
        "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
        "# model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "# processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import zmq\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "context = zmq.Context()\n",
        "socket = context.socket(zmq.REQ)  # REQ (REQUEST) 소켓\n",
        "socket.connect(\"tcp://115.145.175.206:5555\")\n",
        "print(\"Client Start\")\n",
        "action = 0 # oracle_action()\n",
        "\n",
        "action_json = json.dumps(action)\n",
        "socket.send_string(action_json)\n",
        "\n",
        "data = socket.recv_string()\n",
        "data = json.loads(data)\n",
        "\n",
        "rgb_array = np.array(data['rgb'])\n",
        "rgb_path = '/home/shyuni5/file/CORL2024/Sembot/real_bot/save_vision/obs/image_obs.png'\n",
        "cv2.imwrite(rgb_path, rgb_array)\n",
        "print(\"save image!\")\n",
        "\n",
        "image = Image.open(rgb_path).convert(\"RGB\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from custom_utils.llm_utils import *\n",
        "from PIL import Image\n",
        "\n",
        "llm_agent = LLMAgent()\n",
        "image_path = '/home/shyuni5/file/CORL2024/Sembot/real_bot/save_vision/obs/image_obs.png'\n",
        "obs_img = Image.open(image_path)\n",
        "categories_prompt = \"Tell me every objects in the black} table.\"\n",
        "context = llm_agent.gemini_generate_context(categories_prompt, obs_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "text_queries = [\"blue circle plate\", \"yellow circle plate\", \"red circle plate\", \"light green circle plate\", \"yellow block\", \"blue block\", \"red block\", \"green block\", \"green alphabet S\", \"yellow alphabet V\"]\n",
        "print(text_queries)\n",
        "inputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Print input names and shapes\n",
        "for key, val in inputs.items():\n",
        "    print(f\"{key}: {val.shape}\")\n",
        "\n",
        "\n",
        "# text_queries = [ 'letter B', 'letter D', 'letter E', 'letter F', 'letter G', 'letter H',\n",
        "#  'letter J', 'letter K', 'letter L', 'letter M', 'letter O', 'letter V', 'letter R', \n",
        "#   'letter T', 'letter U', 'letter V', 'letter S', 'letter X', 'letter Y', 'letter Z']\n",
        "# text_queries = ['A', 'B','C','D','E','F','G','H','I',\n",
        "#['Circular cap','pill bottle circular cap', 'plastic bottle',]#,'baseball', 'cup', 'dice', 'pencil', 'plastic spoon', 'soda can', 'tennis ball']\n",
        "# text_queries = ['baseball', 'cup', 'dice', 'pencil', 'plastic spoon', 'soda can', 'tennis ball']\n",
        "# text_queries = ['sponge','rectangular prism','red block', 'green block', 'yellow block', 'bottle', 'lotion', 'cup', 'sponge', 'pencil holder', 'yellow pencil',\n",
        "#                 'green basket', 'stain', 'toy car' ,'baseball', 'handcream','pump','shampoo', 'tennis ball' ] #objects # ['G', 'O', 'F', 'D', 'E', 'M', 'R']# , \n",
        "# text_queries = [\"circle shape\", \"rectangle shape\",  \"ring shape\", \"square shape\", \"stick shape\"]\n",
        "# text_queries = [\"trash\", \"snack\", \"food\", \"office supplies\", \"toy\", \"tools\", \"kitchenware\", \"fan\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ten5ZJQsoUbE"
      },
      "source": [
        "## Forward pass\n",
        "\n",
        "Now we can pass the inputs to our OWL-ViT model to get object detection predictions.\n",
        "\n",
        "`OwlViTForObjectDetection` model outputs the prediction logits, boundary boxes and class embeddings, along with the image and text embeddings outputted by the `OwlViTModel`, which is the CLIP backbone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oKBoKQEz_w",
        "outputId": "5deb56c6-c302-4175-a854-d4d6e7b7d903"
      },
      "outputs": [],
      "source": [
        "# Set model in evaluation mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "# Threshold to eliminate low probability predictions\n",
        "score_threshold = 0.19\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers.image_utils import ImageFeatureExtractionMixin\n",
        "mixin = ImageFeatureExtractionMixin()\n",
        "\n",
        "# # Load example image\n",
        "image_size = model.config.vision_config.image_size\n",
        "print(image)\n",
        "image = mixin.resize(image, image_size)\n",
        "input_image = np.asarray(image).astype(np.float32) / 255.0\n",
        "\n",
        "target_sizes = torch.Tensor([image.size[::-1]])\n",
        "print(target_sizes)\n",
        "\n",
        "# Get prediction logits\n",
        "logits = torch.max(outputs[\"logits\"][0], dim=-1)\n",
        "scores = torch.sigmoid(logits.values).cpu().detach().numpy()\n",
        "\n",
        "# Get prediction labels and boundary boxes\n",
        "labels = logits.indices.cpu().detach().numpy()\n",
        "boxes = outputs[\"pred_boxes\"][0].cpu().detach().numpy()\n",
        "# results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=score_threshold)\n",
        "\n",
        "# print(f\"box: {results[0]['boxes'][2]}\")\n",
        "# print(f\"score: {results[0]['scores'][2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2C5dkQ8sBVZ"
      },
      "source": [
        "## Draw predictions on image\n",
        "\n",
        "Let's draw the predictions / found objects on the input image. Remember the found objects correspond to the input text queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "Xc6YjWXBl_vK",
        "outputId": "23ed2b75-59f0-461b-ce8d-48574de96f92"
      },
      "outputs": [],
      "source": [
        "def plot_predictions(input_image, text_queries, scores, boxes, labels, detected_image_path):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    ax.imshow(input_image, extent=(0, 1.0, 0.75, 0))\n",
        "    ax.set_axis_off()\n",
        "    ax.plot(0., 0.0,'ro')\n",
        "    for score, box, label in zip(scores, boxes, labels):\n",
        "      if score < score_threshold:\n",
        "        continue\n",
        "      else:\n",
        "        print(f'label: {text_queries[label]}     score: {score}    box pose: {box}')\n",
        "      cx, cy, w, h = box\n",
        "      cy *= 0.75\n",
        "      h *= 0.75\n",
        "      ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n",
        "              [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n",
        "      ax.text(\n",
        "          cx - w / 2,\n",
        "          cy + h / 2 + 0.015,\n",
        "          f\"{text_queries[label]}: {score:1.2f}\",\n",
        "          ha=\"left\",\n",
        "          va=\"top\",\n",
        "          color=\"red\",\n",
        "          bbox={\n",
        "              \"facecolor\": \"white\",\n",
        "              \"edgecolor\": \"red\",\n",
        "              \"boxstyle\": \"square,pad=.3\"\n",
        "          })\n",
        "\n",
        "    plt.savefig(detected_image_path)\n",
        "detected_image_path = '/home/shyuni5/file/CORL2024/Sembot/real_bot/save_vision/obs/detected_obs.png' \n",
        "resized_image = cv2.resize(input_image, (640, 480))\n",
        "resized_image.shape\n",
        "\n",
        "plot_predictions(resized_image, text_queries, scores, boxes, labels, detected_image_path)    \n",
        "# plot_predictions(input_image, virtualhome_text_queries, scores, boxes, labels, detected_image_path)\n",
        "# X/T/M/B/R/H/U/O/V/Y/S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_points_within_bbox(point_cloud, bbox):\n",
        "    \"\"\"\n",
        "    Extract points from a point cloud that are within the given bounding box.\n",
        "\n",
        "    Parameters:\n",
        "    - point_cloud (np.array): The point cloud as a numpy array of shape (480, 640, 3).\n",
        "    - bbox (tuple): The bounding box specified as (cx, cy, w, h), where cx and cy are the\n",
        "                    coordinates of the center, and w and h are the width and height.\n",
        "\n",
        "    Returns:\n",
        "    - np.array: A filtered point cloud containing only the points within the bounding box.\n",
        "    \"\"\"\n",
        "    cx, cy, w, h = bbox\n",
        "    cx *= 640\n",
        "    cy *= 480\n",
        "    w *= 640\n",
        "    h *= 480\n",
        "    rows, cols, _ = point_cloud.shape\n",
        "\n",
        "    # Calculate the bounds of the bounding box\n",
        "    x_min = max(int(cx - w / 2), 0)\n",
        "    x_max = min(int(cx + w / 2), cols)\n",
        "    y_min = max(int(cy - h / 2), 0)\n",
        "    y_max = min(int(cy + h / 2), rows)\n",
        "\n",
        "    # Create a mask for points within the bounding box\n",
        "    y_indices, x_indices = np.ogrid[:rows, :cols]\n",
        "    mask = (x_indices >= x_min) & (x_indices < x_max) & (y_indices >= y_min) & (y_indices < y_max)\n",
        "\n",
        "    # Filter the point cloud based on the mask\n",
        "    filtered_points = point_cloud[mask]\n",
        "\n",
        "    return filtered_points\n",
        "\n",
        "# g_bbox = [0.88,  0.57,  0.0670583,  0.06833385]\n",
        "# y_bbox = [0.52,  0.57,   0.05145311, 0.06556407]\n",
        "# r_bbox = [0.52,  0.09, 0.05009488, 0.08601852]\n",
        "\n",
        "def transform_coordinates(x, y): # @\n",
        "    # transform pixel pose with robot coordinate\n",
        "    new_x = y + 0.55\n",
        "    new_y = x - 0.13\n",
        "\n",
        "    return new_x, new_y\n",
        "\n",
        "\n",
        "def transform_franka_pose(bbox, pointcloud_array):\n",
        "    filtered_pc = extract_points_within_bbox(pointcloud_array.reshape(rgb_array.shape), bbox)\n",
        "    filtered_pc = filtered_pc[filtered_pc[:, -1] > 0]\n",
        "    sorted_array_by_z = filtered_pc[filtered_pc[:, -1].argsort()]  # Z 값 기준으로 정렬\n",
        "    bottom_10_by_z = sorted_array_by_z[:500]  # Z 값이 가장 작은 하위 10개 선택\n",
        "    selected_point = np.mean(bottom_10_by_z, axis=0)\n",
        "\n",
        "    # min_idx = filtered_pc[:,-1].argmin()\n",
        "    # selected_point = filtered_pc[min_idx]\n",
        "    # argmin: (0.44239964169263835, 0.1612999939918518) \n",
        "    pose = transform_coordinates(selected_point[0], selected_point[1],)\n",
        "    pose_z = 0.863 - selected_point[2]\n",
        "    print(bottom_10_by_z)\n",
        "    return pose[0], pose[1], pose_z \n",
        "\n",
        "bbox =[0.5192626,  0.4542763,  0.05165405, 0.0669348 ]\n",
        "# y_block_bbox = [0.9267224,  0.3878523,  0.03709006, 0.08764208]\n",
        "pose = transform_franka_pose(bbox, pointcloud_array)\n",
        "print(pose)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pointcloud_array.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import os\n",
        "from copy import deepcopy\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "def show_point_cloud(points, color_axis, bbox=None,  width_size=1000, height_size=600, coordinate_frame=True):\n",
        "    '''\n",
        "    points : (N, 3) size of ndarray\n",
        "    color_axis : 0, 1, 2\n",
        "    '''\n",
        "    assert points.shape[1] == 3\n",
        "    assert color_axis==0 or color_axis==1 or color_axis==2   \n",
        "    \n",
        "    min_value = 0.0\n",
        "    max_value = 0.1\n",
        "    # color 값을 해당 범위로 제한\n",
        "    clipped_colors = np.clip(points[:, color_axis], min_value, max_value)\n",
        "\n",
        "    # Create a scatter3d Plotly plot\n",
        "    plotly_fig = go.Figure(data=[go.Scatter3d(\n",
        "        x=points[:, 0],\n",
        "        y=points[:, 1],\n",
        "        z=points[:, 2],\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            size=1,\n",
        "            color=clipped_colors, # Set color based on Z-values\n",
        "            colorscale='jet', # Choose a color scale\n",
        "            colorbar=dict(title='value') # Add a color bar with a title\n",
        "        )\n",
        "    )])\n",
        "\n",
        "    x_range = points[:, 0].max()*0.8 - points[:, 0].min()*0.8\n",
        "    y_range = points[:, 1].max()*0.8 - points[:, 1].min()*0.8\n",
        "    z_range = points[:, 2].max()*0.8 - points[:, 2].min()*0.8\n",
        "\n",
        "    # Adjust the Z-axis scale\n",
        "    plotly_fig.update_layout(\n",
        "        scene=dict(\n",
        "            aspectmode='manual',\n",
        "            aspectratio=dict(x=x_range, y=y_range, z=z_range), # Here you can set the scale of the Z-axis     \n",
        "            camera=dict(\n",
        "                eye=dict(x=0.5, y=0.5, z=0.2)  # 카메라의 위치 조정\n",
        "            )\n",
        "        ),\n",
        "        width=width_size, # Width of the figure in pixels\n",
        "        height=height_size, # Height of the figure in pixels\n",
        "        showlegend=False\n",
        "    )\n",
        "    \n",
        "    if coordinate_frame:\n",
        "        # Length of the axes\n",
        "        axis_length = 1\n",
        "\n",
        "        # Create lines for the axes\n",
        "        lines = [\n",
        "            go.Scatter3d(x=[0, axis_length], y=[0, 0], z=[0, 0], mode='lines', line=dict(color='red')),\n",
        "            go.Scatter3d(x=[0, 0], y=[0, axis_length], z=[0, 0], mode='lines', line=dict(color='green')),\n",
        "            go.Scatter3d(x=[0, 0], y=[0, 0], z=[0, axis_length], mode='lines', line=dict(color='blue'))\n",
        "        ]\n",
        "\n",
        "        # Create cones (arrows) for the axes\n",
        "        cones = [\n",
        "            go.Cone(x=[axis_length], y=[0], z=[0], u=[axis_length], v=[0], w=[0], sizemode='absolute', sizeref=0.1, anchor='tail', showscale=False),\n",
        "            go.Cone(x=[0], y=[axis_length], z=[0], u=[0], v=[axis_length], w=[0], sizemode='absolute', sizeref=0.1, anchor='tail', showscale=False),\n",
        "            go.Cone(x=[0], y=[0], z=[axis_length], u=[0], v=[0], w=[axis_length], sizemode='absolute', sizeref=0.1, anchor='tail', showscale=False)\n",
        "        ]\n",
        "\n",
        "        # Add lines and cones to the figure\n",
        "        for line in lines:\n",
        "            plotly_fig.add_trace(line)\n",
        "        for cone in cones:\n",
        "            plotly_fig.add_trace(cone)\n",
        "\n",
        "\n",
        "    # Add Bounding box in figure\n",
        "    if bbox is not None:\n",
        "        cx,cy,w,h = bbox\n",
        "        bbox_min = [cx-w/2, cy-h/2, 0.0]\n",
        "        bbox_max = [cx+w/2, cy+h/2, 0.1]\n",
        "\n",
        "        bbox_vertices = np.array([\n",
        "            [bbox_min[0], bbox_min[1], bbox_min[2]],\n",
        "            [bbox_min[0], bbox_min[1], bbox_max[2]],\n",
        "            [bbox_min[0], bbox_max[1], bbox_min[2]],\n",
        "            [bbox_min[0], bbox_max[1], bbox_max[2]],\n",
        "            [bbox_max[0], bbox_min[1], bbox_min[2]],\n",
        "            [bbox_max[0], bbox_min[1], bbox_max[2]],\n",
        "            [bbox_max[0], bbox_max[1], bbox_min[2]],\n",
        "            [bbox_max[0], bbox_max[1], bbox_max[2]],\n",
        "        ])\n",
        "\n",
        "        i = [0, 0, 0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 6, 6, 7]\n",
        "        j = [1, 2, 4, 3, 5, 3, 6, 7, 5, 6, 0, 7, 1, 7, 2, 3]\n",
        "        k = [3, 6, 5, 7, 7, 7, 7, 7, 6, 2, 1, 3, 5, 3, 4, 2]\n",
        "        \n",
        "        plotly_fig.add_trace(go.Mesh3d(\n",
        "            x=bbox_vertices[:, 0],\n",
        "            y=bbox_vertices[:, 1],\n",
        "            z=bbox_vertices[:, 2],\n",
        "            i=i,\n",
        "            j=j,\n",
        "            k=k,\n",
        "            color='#FFB6C1',\n",
        "            opacity=0.5\n",
        "        ))\n",
        "\n",
        "\n",
        "    # Show the plot\n",
        "    plotly_fig.show()\n",
        "\n",
        "pc_array = deepcopy(pointcloud_array)\n",
        "pc_array[:,2] = 0.85 - pc_array[:,2] \n",
        "baseball_bbox = [0.68078583, 0.31387693, 0.0932375,  0.1164256 ]\n",
        "tennisball_bbox = [0.7472204,  0.8204705,  0.08430256, 0.11234711]\n",
        "def transform_bbox(bbox):\n",
        "    bbox[0] *= -1\n",
        "    bbox[1] *= -1\n",
        "    bbox[1] *= 0.75\n",
        "    bbox[3] *= 0.75\n",
        "\n",
        "    bbox = [ p * 0.65 for p in bbox]\n",
        "    bbox[0] += 0.6\n",
        "    bbox[1] += 0.1\n",
        "    print(bbox)\n",
        "    return bbox\n",
        "\n",
        "# transform_bbox(baseball_bbox)\n",
        "# transform_bbox(tennisball_bbox)\n",
        "\n",
        "# new_x, new_y = transform_coordinates(baseball_bbox[0], baseball_bbox[1])\n",
        "# baseball_bbox[0] = new_x\n",
        "# baseball_bbox[1] = new_y\n",
        "# 0.17, -0.12\n",
        "# 0 , -0.3\n",
        "show_point_cloud(pc_array, 2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "OWL-ViT-inference example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
